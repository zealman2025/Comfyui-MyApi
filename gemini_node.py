import os
import json
import io

# ä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹é…ç½®
def load_gemini_models_from_config():
    """ä»config.jsonåŠ è½½Geminiæ¨¡å‹é…ç½®"""
    try:
        config_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "config.json")
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            models = config.get('models', {})
            gemini_models = models.get('gemini', {})
            # ç§»é™¤é»˜è®¤æ¨¡å‹å›é€€
            return gemini_models
    except Exception as e:
        print(f"Error loading Gemini models from config: {str(e)}")
        # ä¸å†æä¾›é»˜è®¤æ¨¡å‹
        return {}

# åŠ è½½æ¨¡å‹é…ç½®
GEMINI_MODELS = load_gemini_models_from_config()

class GeminiAINode:
    def __init__(self):
        self.config_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "config.json")
        self.api_key = self._load_api_key()
        
    def _load_api_key(self):
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get('gemini_api_key', '')
        except Exception as e:
            print(f"Error loading Gemini API key: {str(e)}")
            return ''

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": (list(GEMINI_MODELS.keys()),),
                "prompt": ("STRING", {"multiline": True, "default": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œä¸è¦åšå‡ºè¯„è®ºæˆ–å»ºè®®"}),
                "max_tokens": ("INT", {"default": 4096, "min": 1, "max": 8192}),
                "temperature": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": 40, "min": 1, "max": 100}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0x7fffffff}),
            },
            "optional": {
                "image": ("IMAGE",),
                "image_2": ("IMAGE",),
            }
        }

    RETURN_TYPES = ("STRING",)
    FUNCTION = "process"
    CATEGORY = "ğŸMYAPI"

    def process(self, model, prompt, max_tokens=4096, temperature=1.0, top_p=0.95, top_k=40, seed=0, image=None, image_2=None):
        """ä¸»å¤„ç†å‡½æ•°"""
        
        if not self.api_key:
            return ("Error: è¯·åœ¨config.jsonä¸­é…ç½®gemini_api_keyã€‚è¯·è®¿é—® https://aistudio.google.com/ è·å–APIå¯†é’¥ã€‚",)
        
        try:
            # æ£€æŸ¥google-genaiæ˜¯å¦å¯ç”¨
            from google import genai
            from google.genai import types
        except ImportError:
            return ("Error: è¯·å®‰è£…google-genai: pip install google-genai",)
        
        try:
            print(f"Processing request with Gemini model: {model}")
            print(f"Image 1 provided: {image is not None}")
            print(f"Image 2 provided: {image_2 is not None}")
            print(f"Using seed: {seed}")
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            client = genai.Client(api_key=self.api_key)
            
            # å‡†å¤‡å†…å®¹ï¼ˆæŒ‰å®˜æ–¹å»ºè®®ï¼šå•å›¾æ—¶å°†æ–‡æœ¬æ”¾åœ¨å›¾åï¼›å¤šå›¾æ—¶æ–‡æœ¬æ”¾å‰ï¼‰
            contents = []
            
            # å°†å›¾åƒè½¬æ¢ä¸º Gemini Partï¼ˆä¸å®˜æ–¹ç¤ºä¾‹ä¸€è‡´ï¼‰
            def image_to_part(img):
                try:
                    from PIL import Image
                    import numpy as np
                    import torch
                    from google.genai import types as _types
                    
                    # å¼ é‡ -> numpy
                    if isinstance(img, torch.Tensor):
                        if img.is_cuda:
                            img = img.cpu()
                        img = img.numpy()
                    
                    # å¤„ç†æ‰¹ç»´
                    if len(img.shape) == 4:
                        img = img[0]
                    
                    # å½’ä¸€åŒ–åƒç´ 
                    if img.dtype in [np.float32, np.float64] and img.max() <= 1.0:
                        img = (img * 255).astype(np.uint8)
                    
                    # numpy -> PNG bytes
                    pil_image = Image.fromarray(img.astype(np.uint8), 'RGB')
                    buf = io.BytesIO()
                    pil_image.save(buf, format='PNG')
                    image_bytes = buf.getvalue()
                    
                    # æ„å»º Part
                    return _types.Part.from_bytes(data=image_bytes, mime_type='image/png')
                except Exception as e:
                    raise Exception(f"Error converting image to part: {str(e)}")

            # å¤„ç†ç¬¬ä¸€å¼ å›¾åƒï¼ˆä½œä¸º Partï¼‰
            part1 = None
            if image is not None:
                try:
                    part1 = image_to_part(image)
                    print("Successfully converted image 1 to Part")
                except Exception as e:
                    return (f"Error processing image 1: {str(e)}",)

            # å¤„ç†ç¬¬äºŒå¼ å›¾åƒï¼ˆä½œä¸º Partï¼‰
            part2 = None
            if image_2 is not None:
                try:
                    part2 = image_to_part(image_2)
                    print("Successfully converted image 2 to Part")
                except Exception as e:
                    return (f"Error processing image 2: {str(e)}",)

            # ä¾æ®æ˜¯å¦å•å›¾/å¤šå›¾æ„é€  contents é¡ºåº
            if part1 is not None and part2 is not None:
                # å¤šå›¾ï¼šæ–‡æœ¬æ”¾åœ¨æœ€å‰ï¼ˆå‚ç…§å®˜æ–¹å¤šå›¾ç¤ºä¾‹ï¼‰
                contents = [prompt, part1, part2]
            elif part1 is not None:
                # å•å›¾1ï¼šæ–‡æœ¬æ”¾åœ¨å›¾å
                contents = [part1, prompt]
            elif part2 is not None:
                # åªæœ‰å›¾2ï¼šåŒå•å›¾è§„åˆ™
                contents = [part2, prompt]
            else:
                contents = [prompt]

            # ç”Ÿæˆé…ç½®
            config = types.GenerateContentConfig(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_output_tokens=max_tokens,
                seed=seed if seed != 0 and seed <= 0x7fffffff else None,
                response_modalities=["Text"],
                response_mime_type="text/plain"
            )

            # è°ƒç”¨API
            print(f"Calling Gemini API with model: {model}")
            
            response = client.models.generate_content(
                model=model,
                contents=contents,
                config=config
            )
            
            # æ£€æŸ¥å“åº”ç»“æ„å¹¶æå–æ–‡æœ¬
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                
                # æ£€æŸ¥finish_reason
                if hasattr(candidate, 'finish_reason'):
                    finish_reason = str(candidate.finish_reason)
                    if finish_reason == 'MAX_TOKENS':
                        return ("Error: å“åº”å› è¾¾åˆ°æœ€å¤§tokené™åˆ¶è€Œæˆªæ–­ã€‚è¯·å¢åŠ max_tokenså€¼æˆ–ç®€åŒ–æç¤ºè¯ã€‚",)
                    elif finish_reason == 'SAFETY':
                        return ("Error: å“åº”å› å®‰å…¨åŸå› è¢«é˜»æ­¢ã€‚è¯·ä¿®æ”¹æç¤ºè¯å†…å®¹ã€‚",)
                    elif finish_reason == 'RECITATION':
                        return ("Error: å“åº”å› é‡å¤å†…å®¹è¢«æˆªæ–­ã€‚",)
                
                # ä¼˜å…ˆä» finish_message ä¸­æå–
                if hasattr(candidate, 'finish_message') and candidate.finish_message:
                    fm = candidate.finish_message
                    try:
                        # finish_message å¯èƒ½å«æœ‰ content/parts/text
                        if hasattr(fm, 'text') and fm.text:
                            return (fm.text,)
                        if hasattr(fm, 'content') and fm.content:
                            fm_content = fm.content
                            if hasattr(fm_content, 'parts') and fm_content.parts:
                                for p in fm_content.parts:
                                    if hasattr(p, 'text') and p.text:
                                        return (p.text,)
                            if hasattr(fm_content, 'text') and fm_content.text:
                                return (fm_content.text,)
                    except Exception:
                        pass

                if hasattr(candidate, 'content'):
                    content = candidate.content
                    
                    if hasattr(content, 'parts') and content.parts is not None:
                        for part in content.parts:
                            if hasattr(part, 'text') and part.text:
                                return (part.text,)
                    
                    if hasattr(content, 'text'):
                        return (content.text,)
                
                if hasattr(candidate, 'text'):
                    return (candidate.text,)
            
            # å°è¯•ç›´æ¥è®¿é—®response.text
            if hasattr(response, 'text') and response.text:
                return (response.text,)
            
            # SDK è¾…åŠ©æ–¹æ³•å…œåº•
            if hasattr(response, '_get_text'):
                try:
                    _t = response._get_text()
                    if _t:
                        return (_t,)
                except Exception:
                    pass
            
            # parsed å…œåº•
            try:
                if hasattr(response, 'parsed') and response.parsed:
                    parsed_val = response.parsed
                    if isinstance(parsed_val, str) and parsed_val.strip():
                        return (parsed_val,)
            except Exception:
                pass

            # JSON å…œåº•æå–
            try:
                if hasattr(response, 'to_json_dict'):
                    jd = response.to_json_dict()
                    # å¹¿åº¦ä¼˜å…ˆæœç´¢æ‰€æœ‰'text'é”®
                    queue = [jd]
                    while queue:
                        cur = queue.pop(0)
                        if isinstance(cur, dict):
                            if 'text' in cur and isinstance(cur['text'], str) and cur['text'].strip():
                                return (cur['text'],)
                            queue.extend(cur.values())
                        elif isinstance(cur, list):
                            queue.extend(cur)
            except Exception:
                pass
            
            # æœ€ç»ˆå¤±è´¥
            # å°è¯•ç»™å‡º finish_reason æç¤º
            try:
                fr = None
                if hasattr(response, 'candidates') and response.candidates:
                    cand = response.candidates[0]
                    if hasattr(cand, 'finish_reason'):
                        fr = str(cand.finish_reason)
                if fr:
                    return (f"Error: æ— æ³•æå–æ–‡æœ¬ï¼ˆfinish_reason={fr}ï¼‰ã€‚",)
            except Exception:
                pass
            return ("Error: æ— æ³•ä»å“åº”ä¸­æå–æ–‡æœ¬å†…å®¹ã€‚",)
            
        except Exception as e:
            print(f"Unexpected error in Gemini process: {str(e)}")
            return (f"Error: {str(e)}",)

NODE_CLASS_MAPPINGS = {
    "GeminiAINode": GeminiAINode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiAINode": "ğŸŒŸGemini AI"
}